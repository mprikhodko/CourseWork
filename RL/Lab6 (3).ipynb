{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "#  Foundations of Reinforcement Learning\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font color=\"darkblue\">Lab 6: On-policy Control with Function Approximation </font></h1>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##  Content\n",
    "1. Episodic Semi-gradient SARSA\n",
    "2. Differencial Semi-gradient SARSA for Continuing Problem\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Gym and other necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import itertools\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "import timeit\n",
    "from collections import namedtuple\n",
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "import io\n",
    "import base64\n",
    "\n",
    "\n",
    "import Tilecoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please examine and practice \"Tilecoding.py\" and the following code carefully to get start with tile-coding as well as predict and update in q-function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QEstimator():\n",
    "    \"\"\"\n",
    "    Linear action-value (q-value) function approximator for \n",
    "    semi-gradient methods with state-action featurization via tile coding. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, step_size, num_tilings=8, max_size=4096, tiling_dim=None):\n",
    "        \n",
    "\n",
    "        self.max_size = max_size\n",
    "        self.num_tilings = num_tilings\n",
    "        self.tiling_dim = tiling_dim or num_tilings\n",
    "\n",
    "        # Step size is interpreted as the fraction of the way we want \n",
    "        # to move towards the target. To compute the learning rate alpha,\n",
    "        # scale by number of tilings. \n",
    "        self.alpha = step_size / num_tilings\n",
    "\n",
    "        # Initialize index hash table (IHT) for tile coding.\n",
    "        # This assigns a unique index to each tile up to max_size tiles.\n",
    "        # Ensure max_size >= total number of tiles (num_tilings x tiling_dim x tiling_dim)\n",
    "        # to ensure no duplicates.\n",
    "        self.iht = Tilecoding.IHT(max_size)\n",
    "\n",
    "\n",
    "        self.weights = np.zeros(max_size)\n",
    "\n",
    "\n",
    "        # Tilecoding software partitions at integer boundaries, so must rescale\n",
    "        # position and velocity space to span tiling_dim x tiling_dim region.\n",
    "        self.position_scale = self.tiling_dim / (env.observation_space.high[2] \\\n",
    "                                                  - env.observation_space.low[2])\n",
    "        self.velocity_scale = self.tiling_dim / (env.observation_space.high[2]*2 \\\n",
    "                                                  - env.observation_space.low[2]*2)\n",
    "        \n",
    "    def featurize_state_action(self, state, action):\n",
    "        \"\"\"\n",
    "        Returns the featurized representation for a \n",
    "        state-action pair.\n",
    "        \"\"\"\n",
    "        featurized = Tilecoding.tiles(self.iht, self.num_tilings, \n",
    "                           [self.position_scale * state[2], \n",
    "                            self.velocity_scale * state[3]], \n",
    "                           [action])\n",
    "        return featurized\n",
    "    \n",
    "    def predict(self, s, a=None):\n",
    "        \"\"\"\n",
    "        Predicts q-value(s) using linear FA.\n",
    "        If action a is given then returns prediction\n",
    "        for single state-action pair (s, a).\n",
    "        Otherwise returns predictions for all actions \n",
    "        in environment paired with s.   \n",
    "        \"\"\"\n",
    "    \n",
    "        if a is None:\n",
    "            features = [self.featurize_state_action(s, i) for \n",
    "                        i in range(env.action_space.n)]\n",
    "        else:\n",
    "            features = [self.featurize_state_action(s, a)]\n",
    "            \n",
    "        return [np.sum(self.weights[f]) for f in features]\n",
    "        \n",
    "            \n",
    "    def update(self, s, a, target):\n",
    "        \"\"\"\n",
    "        Updates the estimator parameters\n",
    "        for a given state and action towards\n",
    "        the target using the gradient update rule \n",
    "        \"\"\"\n",
    "        features = self.featurize_state_action(s, a)\n",
    "        estimation = np.sum(self.weights[features])  # Linear FA\n",
    "        delta = (target - estimation)\n",
    "        \n",
    "\n",
    "        self.weights[features] += self.alpha * delta\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(estimator, epsilon, num_actions):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a \n",
    "    given q-value approximator and epsilon.    \n",
    "    \"\"\"\n",
    "    def policy_fn(observation):\n",
    "        action_probs = np.ones(num_actions, dtype=float) * epsilon / num_actions\n",
    "        q_values = estimator.predict(observation)\n",
    "        best_action_idx = np.argmax(q_values)\n",
    "        action_probs[best_action_idx] += (1.0 - epsilon)\n",
    "        return action_probs\n",
    "    return policy_fn\n",
    "\n",
    "def get_epsilon(t):\n",
    "    return max(0.1, min(1., 1. - np.log10((t + 1) / 25)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Episodic Semi-gradient SARSA\n",
    "1. Apply Episodic Semi-gradient SARSA (See Sutton&Barto Section 10.1) to the carpole example for 500 episodes to obtain an approximate optimal policy. \n",
    "0. Divide the total 500 episodes into 10 sets. Plot the average reward for each set. (i.e. plot the average reward for the first 50 episodes, the second 50 episodes, ..., and the 10th 50 episodes.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Episodic Semi-gradient SARSA\n",
    "## Suggested flow: try to complete the algorithm with functions above (OR feel free to modify, add and use other tool)\n",
    "\n",
    "\n",
    "total_reward = 0\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "observation = env.reset()\n",
    "if gym.__version__>'0.26.0':\n",
    "    observation = observation[0]\n",
    "\n",
    "\n",
    "step_size = 0.5  # Fraction of the way we want to move towards target\n",
    "num_episodes = 500\n",
    "set_size = 50\n",
    "\n",
    "# initialize QEstimator\n",
    "estimator = QEstimator(step_size=step_size)\n",
    "\n",
    "\n",
    "\n",
    "gamma = 0.98\n",
    "result = np.zeros(10)\n",
    "s = 0\n",
    "for ep in range(num_episodes):\n",
    "    if  np.mod(ep,set_size)==0:\n",
    "        print(\"Finishing set:\",s)\n",
    "        result[s] = total_reward/set_size\n",
    "        s+=1\n",
    "        total_reward = 0\n",
    "\n",
    "    epsilon = get_epsilon(ep)\n",
    "    \n",
    "    # get epsilon-greedy policy\n",
    "    policy = make_epsilon_greedy_policy(\n",
    "        estimator, epsilon, env.action_space.n)\n",
    "    \n",
    "    # end of one episode \n",
    "    state = env.reset()\n",
    "    if gym.__version__>'0.26.0':\n",
    "        observation = observation[0]\n",
    "    done = False\n",
    "    \n",
    "    \n",
    "    action =\n",
    "    \n",
    "    while not done:\n",
    "        # during one episode\n",
    "        total_reward += 1\n",
    "        #################### simulate one step\n",
    "        if gym.__version__>'0.26.0':\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "        else:\n",
    "            observation, reward, done, info = env.step(action) \n",
    "        ####################\n",
    "        next_action = \n",
    "        \n",
    "        \n",
    "        state = next_state\n",
    "        action = next_action\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Differencial Semi-gradient SARSA\n",
    "\n",
    "Now we view the carpole problem as a continuing problem: In the carpole environment, whenever the agent reaches a non-terminal states, it receives +1 reward; whenever the agent reaches a terminal states, it receives 0 reward, and move to a non-terminal state by reseting the enviornment.\n",
    "\n",
    "1. Apply Differencial Semi-gradient SARSA (See Sutton&Barto Section 10.3) to this modified carpole example for 500 episodes to obtain an approximate optimal policy. \n",
    "0. Divide the total 500 episodes into 10 sets. Plot the average reward for each set. (i.e. plot the average reward for the first 50 episodes, the second 50 episodes, ..., and the 10th 50 episodes.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Differencial Semi-gradient SARSA\n",
    "## Suggested flow: try to complete the algorithm with functions above (OR feel free to modify, add and use other tool)\n",
    "\n",
    "\n",
    "total_reward = 0\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "observation = env.reset()\n",
    "if gym.__version__>'0.26.0':\n",
    "    observation = observation[0]\n",
    "\n",
    "\n",
    "step_size = 0.5  # Fraction of the way we want to move towards target\n",
    "num_episodes = 500\n",
    "set_size = 50\n",
    "\n",
    "# initialize QEstimator\n",
    "estimator = QEstimator(step_size=step_size)\n",
    "\n",
    "\n",
    "\n",
    "gamma = 0.98\n",
    "result = np.zeros(10)\n",
    "s = 0\n",
    "for ep in range(num_episodes):\n",
    "    if  np.mod(ep,set_size)==0:\n",
    "        print(\"Finishing set:\",s)\n",
    "        result[s] = total_reward/set_size\n",
    "        s+=1\n",
    "        total_reward = 0\n",
    "\n",
    "    epsilon = get_epsilon(ep)\n",
    "        \n",
    "    policy = make_epsilon_greedy_policy(\n",
    "        estimator, epsilon, env.action_space.n)\n",
    "    \n",
    "    # end of episode\n",
    "    state = env.reset()\n",
    "    if gym.__version__>'0.26.0':\n",
    "        observation = observation[0]\n",
    "    done = False\n",
    "    \n",
    "    action = \n",
    "\n",
    "    \n",
    "    while not done:\n",
    "        # during one episode\n",
    "        total_reward += 1\n",
    "        #################### simulate one step\n",
    "        if gym.__version__>'0.26.0':\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "        else:\n",
    "            observation, reward, done, info = env.step(action) \n",
    "        ####################\n",
    "        next_action = \n",
    "        \n",
    "        \n",
    "        state = next_state\n",
    "        action = next_action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
