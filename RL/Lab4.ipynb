{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "# Foundations of Reinforcement Learning\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font color=\"darkblue\">Lab 4: Monte Carlo Method  </font></h1>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##  Content\n",
    "1. Monte Carlo Method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Gym and other necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from IPython import display\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Monte Carlo Method  (CartPole-v1 environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 CartPole Introduction\n",
    "\n",
    "We now apply Monte Carlo Method for CartPole problem. \n",
    "\n",
    "\n",
    "1. A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. \n",
    "\n",
    "0. The system is controlled by applying a force of +1 or -1 to the cart. \n",
    "\n",
    "0. The pendulum starts up, and the goal is to prevent it from falling over. \n",
    "\n",
    "0. A reward of +1 is provided for every timestep that the pole remains up. \n",
    "\n",
    "0. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\n",
    "\n",
    "0. For more info (See [SOURCE ON GITHUB](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py)).\n",
    "\n",
    "The following examples show the basic usage of this testing environment: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Episode initialization and Initial Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inital observation is [ 0.03085803 -0.04887363 -0.02494748  0.026118  ]\n",
      "\n",
      "This means the cart current position is 0.030858034268021584 with velocity -0.04887362942099571,\n",
      "and the pole current angular position is -0.024947484955191612 with angular velocity 0.026118000969290733,\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "observation = env.reset() ##Initial an episode\n",
    "\n",
    "if gym.__version__>'0.26.0':\n",
    "    observation = observation[0]\n",
    "\n",
    "print(\"Inital observation is {}\".format(observation))\n",
    "\n",
    "print(\"\\nThis means the cart current position is {}\".format(observation[0]), end = '')\n",
    "print(\" with velocity {},\".format(observation[1]))\n",
    "\n",
    "print(\"and the pole current angular position is {}\".format(observation[2]), end = '')\n",
    "print(\" with angular velocity {},\".format(observation[3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Take actions\n",
    "\n",
    "\n",
    "Use env.step(action) to take an action\n",
    "\n",
    "action is an integer from 0 to 1\n",
    "\n",
    "0: \"Left\"; 1: \"Right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current observation is [ 0.03085803 -0.04887363 -0.02494748  0.026118  ]\n",
      "\n",
      "New observation is [ 0.02988056 -0.2436291  -0.02442512  0.3108265 ]\n",
      "Step reward is 1.0\n",
      "Did episode just ends? -False\n"
     ]
    }
   ],
   "source": [
    "print(\"Current observation is {}\".format(observation))\n",
    "\n",
    "action = 0 #go left\n",
    "\n",
    "#################### simulate one step\n",
    "if gym.__version__>'0.26.0':\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "else:\n",
    "    observation, reward, done, info = env.step(action) \n",
    "####################\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nNew observation is {}\".format(observation))\n",
    "print(\"Step reward is {}\".format(reward))\n",
    "print(\"Did episode just ends? -{}\".format(done)) # episode ends when 3.1(6) happens\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Simulate multiple episodes\n",
    "\n",
    "(You may uncomment those lines to see an animation. However, it will not work for JupyterHub since the animation requires GL instead of webGL. If you have Jupyter notebook localy on your computer, this version of code will work through a virtual frame.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "observation = env.reset()\n",
    "total_reward = 0\n",
    "ep_num = 0\n",
    "# img = plt.imshow(env.render(mode='rgb_array')) \n",
    "\n",
    "\n",
    "for _ in range(1000):\n",
    "    #     img.set_data(env.render(mode='rgb_array')) \n",
    "    #     display.display(plt.gcf())\n",
    "    #     display.clear_output(wait=True)\n",
    "    \n",
    "    action = env.action_space.sample()     # this takes random actions\n",
    "    #################### simulate one step\n",
    "    if gym.__version__>'0.26.0':\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "    else:\n",
    "        observation, reward, done, info = env.step(action) \n",
    "    ####################\n",
    "       \n",
    "    total_reward += reward\n",
    "    \n",
    "\n",
    "\n",
    "    if done:                               # episode just ends\n",
    "        observation = env.reset()          # reset episode\n",
    "        if gym.__version__>'0.26.0':\n",
    "            observation = observation[0]\n",
    "        ep_num += 1\n",
    "\n",
    "print(\"Average reward per episode is {}\".format(total_reward/ep_num))\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 States simplification \n",
    "\n",
    "For convenience, we consider only cart position and pole angular position, (i.e. state dimension = 2).\n",
    "\n",
    "Note that the observed cart position $P \\in [-4.8, 4.8]$ and pole angular position $\\theta \\in [-0.418, 0.418]$ for all times. Then, we could evenly devide those two intervals to from a finite number of states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_state_idx(ob,ls0,ls1):\n",
    "    pos_diff = ob[0] +4.8\n",
    "    a_pos_diff = ob[2] + 0.418\n",
    "    \n",
    "    step_size_1 = 4.8*2/(ls0-1)\n",
    "    step_size_2 = 0.418*2/(ls1-1)\n",
    "    \n",
    "    \n",
    "    d_1 = np.round(pos_diff/step_size_1)\n",
    "    d_3 = np.round(a_pos_diff/step_size_2)\n",
    "     \n",
    "    return [d_1,d_3]\n",
    "\n",
    "\n",
    "ls_cart = 100 #devide the position of cart into 100 states\n",
    "ls_pole = 100 #devide the angular position of pole into 100 states\n",
    "\n",
    "# Threre are 100 * 100 = 10000 different states in total\n",
    "\n",
    "observation = env.reset()\n",
    "if gym.__version__>'0.26.0':\n",
    "    observation = observation[0]\n",
    "state_idx = find_state_idx(observation,ls_cart,ls_pole)\n",
    "\n",
    "print(\"\\nThe cart current position is {}\".format(observation[0]), end = '')\n",
    "print(\"and the pole current angular position is {}\".format(observation[2]))\n",
    "\n",
    "print(\"which projected to state {}\".format(state_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 On-policy first-visit MC control\n",
    "1. Implement \"On-policy first-visit MC control\" algorithum in [Ch 5.4 Sutton] to choose optimal actions\n",
    "2. Simulate this algorithum for 30000 episodes.\n",
    "3. Devide the previous 30000 episodes into 15 sets. Plot average rewards for each sets. (i.e. plot average rewards for the first 2000 episodes, the second 2000 episodes, ..., and the 15th 2000 episodes.) \n",
    "4. Plot the heatmap for Q for each action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Suggested functions (Feel free to modify existing and add new functions)\n",
    "\n",
    "def get_action(current_state, Q, epsilon):\n",
    "    \n",
    "    # Choose optimal action based on current state and Q\n",
    "    #\n",
    "    # input:  current state,  (array) \n",
    "    #         Q,              (array)  \n",
    "    #         epsilonn,       (float)  \n",
    "    # output: action\n",
    "    #         \n",
    "    return action\n",
    "\n",
    "\n",
    "\n",
    "def update_Q(Q, obs, acts):\n",
    "    # Update Q at the end of each episode\n",
    "    #\n",
    "    # input:  current Q, (array) \n",
    "    #         obs,       (array)  states observed in this episode\n",
    "    #         acts,       (array)  actions took in this spisode\n",
    "    # output: Updated Q\n",
    "    #         \n",
    "\n",
    "        \n",
    "    return Q\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Suggested flow (Feel free to modify and add)\n",
    "\n",
    "\n",
    "set_num = 15\n",
    "s = 0\n",
    "env = gym.make('CartPole-v1')\n",
    "observation = env.reset()\n",
    "if gym.__version__>'0.26.0':\n",
    "    observation = observation[0]\n",
    "while 1:\n",
    "    \n",
    "    current_state = observation\n",
    "    action = get_action(current_state,Q,epsilon)\n",
    "    \n",
    "    #################### simulate one step\n",
    "    if gym.__version__>'0.26.0':\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "    else:\n",
    "        observation, reward, done, info = env.step(action) \n",
    "    ####################\n",
    "    \n",
    "    if done:  # end of epsode\n",
    "        Q = update_Q(Q, obs, acts)\n",
    "        \n",
    "        ep_num += 1\n",
    "        \n",
    "        if  np.mod(ep_num,2000)==0: # end of every set of episode\n",
    "            s+=1\n",
    "            \n",
    "            if s == set_num:\n",
    "                break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
